import streamlit as st
import requests
import re
import json
from bs4 import BeautifulSoup
import time
import os
from openai import OpenAI
import codecs

st.set_page_config(page_title="å°çº¢ä¹¦æ–‡å­—æå–å·¥å…·", page_icon="ğŸ“", layout="wide")

st.title("ğŸ“ å°çº¢ä¹¦æ–‡å­—æå–å·¥å…·")
st.markdown("âœ¨ ä¸“é—¨æå–å°çº¢ä¹¦å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹")

# ä¾§è¾¹æ è®¾ç½®
with st.sidebar:
    st.header("âš™ï¸ è®¾ç½®")
    user_agent = st.text_input(
        "User-Agent (å¯é€‰)", 
        value="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        help="è‡ªå®šä¹‰User-Agentï¼Œç”¨äºæ¨¡æ‹Ÿæµè§ˆå™¨è¯·æ±‚"
    )
    
    st.markdown("---")
    st.subheader("ğŸ” OCRè®¾ç½®")
    
    # è®¾ç½®API Key
    ark_api_key = st.text_input(
        "è±†åŒ…API Key", 
        value="79329882-cb1a-4b0a-be47-765e83281ea4",
        type="password",
        help="è±†åŒ…API Keyï¼Œç”¨äºè°ƒç”¨è§†è§‰å¤§æ¨¡å‹"
    )
    
    # æ¨¡å‹é€‰æ‹©
    model_id = st.text_input(
        "è±†åŒ…æ¨¡å‹ID",
        value="ep-20250512154520-xv6s9",
        help="è±†åŒ…æ¨ç†æ¥å…¥ç‚¹ID"
    )

# URLæ¸…ç†å‡½æ•°
def clean_image_url(url):
    """æ¸…ç†å’Œè§£ç å›¾ç‰‡URL"""
    if not url:
        return ""
    
    try:
        # ç§»é™¤URLå‰çš„@ç¬¦å·
        url = url.lstrip('@').strip()
        
        # å¤„ç†Unicodeè½¬ä¹‰å­—ç¬¦
        if '\\u' in url:
            url = codecs.decode(url, 'unicode_escape')
        
        # å¦‚æœæ˜¯JSONå­—ç¬¦ä¸²ï¼Œå°è¯•è§£æ
        if url.startswith('{') and url.endswith('}'):
            try:
                json_obj = json.loads(url)
                if 'urlDefault' in json_obj:
                    url = json_obj['urlDefault']
                elif 'url' in json_obj:
                    url = json_obj['url']
                # å†æ¬¡å¤„ç†è½¬ä¹‰å­—ç¬¦
                if '\\u' in url:
                    url = codecs.decode(url, 'unicode_escape')
            except:
                pass
        
        # ç¡®ä¿URLæ˜¯httpå¼€å¤´
        if url.startswith('//'):
            url = 'http:' + url
        elif not url.startswith(('http://', 'https://')):
            url = 'http://' + url
        
        return url
    except:
        return url

# åˆå§‹åŒ–è±†åŒ…å®¢æˆ·ç«¯
def init_doubao_client(api_key):
    """åˆå§‹åŒ–è±†åŒ…è§†è§‰å¤§æ¨¡å‹å®¢æˆ·ç«¯"""
    try:
        client = OpenAI(
            base_url="https://ark.cn-beijing.volces.com/api/v3",
            api_key=api_key,
        )
        return client
    except Exception as e:
        st.error(f"è±†åŒ…å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        return None

# ä¸»ç•Œé¢
st.markdown("### ğŸ“ è¯·è¾“å…¥å°çº¢ä¹¦é“¾æ¥")
xhs_url = st.text_input(
    "å°çº¢ä¹¦é“¾æ¥",
    placeholder="https://www.xiaohongshu.com/explore/...",
    help="æ”¯æŒå°çº¢ä¹¦ç¬”è®°é“¾æ¥"
)

# æå–å°çº¢ä¹¦IDçš„å‡½æ•°
def extract_note_id(url):
    """ä»å°çº¢ä¹¦é“¾æ¥ä¸­æå–ç¬”è®°ID"""
    patterns = [
        r'explore/([a-f0-9]+)',
        r'discovery/item/([a-f0-9]+)',
        r'/([a-f0-9]{24})',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    return None

# ä½¿ç”¨è±†åŒ…APIè¯†åˆ«å›¾ç‰‡æ–‡å­—
def extract_text_from_image_doubao(img_url, client, model_id):
    """ä½¿ç”¨è±†åŒ…è§†è§‰å¤§æ¨¡å‹ä»å›¾ç‰‡ä¸­æå–æ–‡å­—"""
    if client is None:
        return "è±†åŒ…å®¢æˆ·ç«¯æœªåˆå§‹åŒ–"
    
    try:
        response = client.chat.completions.create(
            model=model_id,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": img_url
                            },
                        },
                        {
                            "type": "text", 
                            "text": "è¯·è¯†åˆ«å¹¶æå–è¿™å¼ å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—å†…å®¹ï¼ŒåŒ…æ‹¬ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ç­‰ã€‚è¯·æŒ‰ç…§å›¾ç‰‡ä¸­æ–‡å­—çš„å¸ƒå±€é¡ºåºï¼Œé€è¡Œè¿”å›è¯†åˆ«çš„æ–‡å­—ï¼Œä¿æŒåŸæœ‰çš„æ¢è¡Œç»“æ„ã€‚å¦‚æœæ²¡æœ‰æ–‡å­—å°±è¿”å›'æ— æ–‡å­—'ã€‚"
                        },
                    ],
                }
            ],
        )
        
        if response.choices and response.choices[0].message:
            text_content = response.choices[0].message.content.strip()
            return text_content if text_content and text_content != "æ— æ–‡å­—" else "æœªè¯†åˆ«åˆ°æ–‡å­—"
        else:
            return "APIå“åº”ä¸ºç©º"
            
    except Exception as e:
        return f"è±†åŒ…APIè°ƒç”¨é”™è¯¯: {str(e)}"

# æŠ“å–å°çº¢ä¹¦å†…å®¹çš„å‡½æ•°
def fetch_xhs_content(url):
    """æŠ“å–å°çº¢ä¹¦å†…å®¹"""
    try:
        headers = {
            'User-Agent': user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
            'sec-ch-ua': '"Google Chrome";v="119", "Chromium";v="119", "Not?A_Brand";v="24"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"macOS"'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        # è§£æHTML
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # å°è¯•ä»ä¸åŒä½ç½®æå–å†…å®¹
        content_data = {}
        
        # æå–æ ‡é¢˜
        title_selectors = [
            'meta[property="og:title"]',
            'title',
            '.note-title',
            'h1'
        ]
        
        for selector in title_selectors:
            title_element = soup.select_one(selector)
            if title_element:
                if selector.startswith('meta'):
                    content_data['title'] = title_element.get('content', '').strip()
                else:
                    content_data['title'] = title_element.get_text().strip()
                if content_data['title']:
                    break
        
        # æå–æè¿°/å†…å®¹
        desc_selectors = [
            'meta[property="og:description"]',
            'meta[name="description"]',
            '.note-content',
            '.desc'
        ]
        
        for selector in desc_selectors:
            desc_element = soup.select_one(selector)
            if desc_element:
                if selector.startswith('meta'):
                    content_data['description'] = desc_element.get('content', '').strip()
                else:
                    content_data['description'] = desc_element.get_text().strip()
                if content_data['description']:
                    break
        
        # æå–å›¾ç‰‡ - å¢å¼ºç‰ˆ
        img_selectors = [
            'meta[property="og:image"]',
            'meta[name="twitter:image"]',
            '.note-image img',
            '.image-container img',
            '.photo-item img',
            '.swiper-slide img',
            'img[src*="ci.xiaohongshu.com"]',
            'img[src*="sns-webpic-qc.xhscdn.com"]',
            'img[src*="xhscdn.com"]',
            'img[data-src*="xhscdn.com"]',
            'img[data-original*="xhscdn.com"]',
            'img'  # ä½œä¸ºå¤‡é€‰ï¼Œè·å–æ‰€æœ‰imgæ ‡ç­¾
        ]
        
        images = []
        
        for selector in img_selectors:
            if selector.startswith('meta'):
                img_element = soup.select_one(selector)
                if img_element:
                    img_url = img_element.get('content', '')
                    if img_url and 'xhs' in img_url.lower():
                        cleaned_url = clean_image_url(img_url)
                        images.append(cleaned_url)
            else:
                img_elements = soup.select(selector)
                for img in img_elements:
                    # å°è¯•å¤šä¸ªå±æ€§è·å–å›¾ç‰‡URL
                    img_url = (img.get('src', '') or 
                              img.get('data-src', '') or 
                              img.get('data-original', '') or
                              img.get('data-lazy', '') or
                              img.get('data-url', ''))
                    
                    if img_url:
                        # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯å†…å®¹å›¾ç‰‡çš„URL
                        if ('xhs' in img_url.lower() and
                            not any(x in img_url.lower() for x in ['avatar', 'head', 'icon', 'logo', 'badge'])):
                            cleaned_url = clean_image_url(img_url)
                            if cleaned_url not in images:
                                images.append(cleaned_url)
        
        content_data['images'] = images[:10]  # é™åˆ¶æœ€å¤š10å¼ å›¾ç‰‡
        
        # å°è¯•ä»é¡µé¢æºç ä¸­æå–JSONæ•°æ®ä»¥è·å–æ›´å¤šå›¾ç‰‡
        script_tags = soup.find_all('script')
        json_images_found = []
        
        for script in script_tags:
            if script.string:
                # å°è¯•å¤šç§JSONæ•°æ®æå–æ–¹å¼
                patterns = [
                    r'window\.__INITIAL_STATE__\s*=\s*({.+?});',
                    r'window\.__NUXT__\s*=\s*({.+?});',
                    r'"imageList"\s*:\s*\[([^\]]+)\]',
                    r'"urlDefault"\s*:\s*"([^"]+)"',
                    r'"url"\s*:\s*"(https://[^"]*xhscdn[^"]*)"'
                ]
                
                for pattern in patterns:
                    matches = re.findall(pattern, script.string)
                    for match in matches:
                        if pattern == patterns[0] or pattern == patterns[1]:  # å®Œæ•´JSON
                            try:
                                data = json.loads(match)
                                # é€’å½’æœç´¢å›¾ç‰‡URL
                                def find_images_in_json(obj, path=""):
                                    if isinstance(obj, dict):
                                        for key, value in obj.items():
                                            if key in ['urlDefault', 'url', 'src'] and isinstance(value, str) and 'xhscdn' in value:
                                                cleaned_url = clean_image_url(value)
                                                json_images_found.append(cleaned_url)
                                                if cleaned_url not in content_data['images']:
                                                    content_data['images'].append(cleaned_url)
                                            else:
                                                find_images_in_json(value, f"{path}.{key}")
                                    elif isinstance(obj, list):
                                        for i, item in enumerate(obj):
                                            find_images_in_json(item, f"{path}[{i}]")
                                
                                find_images_in_json(data)
                            except:
                                continue
                        else:  # ç›´æ¥URLåŒ¹é…
                            if 'xhscdn' in match:
                                cleaned_url = clean_image_url(match)
                                if cleaned_url not in content_data['images']:
                                    json_images_found.append(cleaned_url)
                                    content_data['images'].append(cleaned_url)
        
        # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥ä»é¡µé¢æºç ä¸­æœç´¢æ‰€æœ‰å¯èƒ½çš„å›¾ç‰‡URL
        if len(content_data['images']) == 0:
            backup_images = []
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æœç´¢æ‰€æœ‰åŒ…å«xhscdnçš„URL
            url_patterns = [
                r'https://[^"\s]*xhscdn[^"\s]*\.(?:jpg|jpeg|png|webp|gif)',
                r'https://[^"\s]*ci\.xiaohongshu[^"\s]*\.(?:jpg|jpeg|png|webp|gif)',
                r'"(https://[^"]*xhscdn[^"]*)"',
                r"'(https://[^']*xhscdn[^']*)'"
            ]
            
            page_text = response.text
            for pattern in url_patterns:
                matches = re.findall(pattern, page_text, re.IGNORECASE)
                for match in matches:
                    cleaned_url = clean_image_url(match)
                    if cleaned_url not in backup_images and len(backup_images) < 10:
                        backup_images.append(cleaned_url)
            
            if backup_images:
                content_data['images'].extend(backup_images[:5])  # æ·»åŠ å‰5ä¸ªå¤‡ç”¨å›¾ç‰‡
        
        return content_data
        
    except requests.RequestException as e:
        return {'error': f'ç½‘ç»œè¯·æ±‚é”™è¯¯: {str(e)}'}
    except Exception as e:
        return {'error': f'è§£æé”™è¯¯: {str(e)}'}

# å¤„ç†é“¾æ¥è¾“å…¥
if xhs_url:
    if not xhs_url.startswith(('http://', 'https://')):
        xhs_url = 'https://' + xhs_url
    
    # éªŒè¯æ˜¯å¦ä¸ºå°çº¢ä¹¦é“¾æ¥
    if 'xiaohongshu.com' not in xhs_url and 'xhslink.com' not in xhs_url:
        st.error("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„å°çº¢ä¹¦é“¾æ¥")
    else:
        # æå–ç¬”è®°ID
        note_id = extract_note_id(xhs_url)
        if note_id:
            st.info(f"ğŸ“‹ æ£€æµ‹åˆ°ç¬”è®°ID: {note_id}")
        
        # å¼€å§‹æŠ“å–æŒ‰é’®
        if st.button("ğŸš€ å¼€å§‹æå–æ–‡å­—", type="primary"):
            # æ£€æŸ¥å¿…è¦å‚æ•°
            if not ark_api_key:
                st.error("âŒ è¯·è¾“å…¥è±†åŒ…API Key")
            elif not model_id:
                st.error("âŒ è¯·è¾“å…¥è±†åŒ…æ¨¡å‹ID")
            else:
                # åˆå§‹åŒ–è±†åŒ…å®¢æˆ·ç«¯
                with st.spinner("æ­£åœ¨åˆå§‹åŒ–è±†åŒ…å®¢æˆ·ç«¯..."):
                    doubao_client = init_doubao_client(ark_api_key)
                
                if doubao_client:
                    with st.spinner("æ­£åœ¨æŠ“å–å†…å®¹ï¼Œè¯·ç¨å€™..."):
                        content = fetch_xhs_content(xhs_url)
                        
                        if 'error' in content:
                            st.error(f"æŠ“å–å¤±è´¥: {content['error']}")
                            st.markdown("""
                            ### ğŸ’¡ æŠ“å–å»ºè®®
                            - ç¡®ä¿é“¾æ¥æœ‰æ•ˆä¸”å¯è®¿é—®
                            - å°çº¢ä¹¦å¯èƒ½æœ‰åçˆ¬è™«æœºåˆ¶ï¼Œè¯·ç¨åé‡è¯•
                            - å°è¯•ä½¿ç”¨ä¸åŒçš„User-Agent
                            - æŸäº›ç§å¯†å†…å®¹å¯èƒ½æ— æ³•è®¿é—®
                            """)
                        else:
                            st.success("âœ… æŠ“å–æˆåŠŸ!")
                            
                            # æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
                            st.markdown("---")
                            st.markdown("### ğŸ“„ åŸºæœ¬ä¿¡æ¯")
                            
                            col1, col2 = st.columns([2, 1])
                            
                            with col1:
                                # æ˜¾ç¤ºæ ‡é¢˜
                                if content.get('title'):
                                    st.markdown("#### ğŸ“ æ ‡é¢˜")
                                    st.write(content['title'])
                                
                                # æ˜¾ç¤ºæè¿°/å†…å®¹
                                if content.get('description'):
                                    st.markdown("#### ğŸ“– å†…å®¹æè¿°")
                                    st.write(content['description'])
                            
                            with col2:
                                st.markdown("#### ğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
                                st.metric("å›¾ç‰‡æ•°é‡", len(content.get('images', [])))
                                if content.get('title'):
                                    st.metric("æ ‡é¢˜é•¿åº¦", len(content['title']))
                                if content.get('description'):
                                    st.metric("æè¿°é•¿åº¦", len(content['description']))
                            
                            # å›¾ç‰‡æ–‡å­—æå–
                            if content.get('images'):
                                st.markdown("---")
                                st.markdown("### ğŸ” å›¾ç‰‡æ–‡å­—æå–")
                                st.write(f"å…±å‘ç° {len(content['images'])} å¼ å›¾ç‰‡ï¼Œæ­£åœ¨æå–æ–‡å­—...")
                                
                                # OCRè¯†åˆ«è¿›åº¦æ¡
                                progress_bar = st.progress(0)
                                status_text = st.empty()
                                
                                all_ocr_texts = []  # å­˜å‚¨æ‰€æœ‰OCRè¯†åˆ«çš„æ–‡å­—
                                
                                # å¤„ç†æ¯å¼ å›¾ç‰‡
                                for idx, img_url in enumerate(content['images']):
                                    # æ›´æ–°è¿›åº¦
                                    progress = (idx + 1) / len(content['images'])
                                    progress_bar.progress(progress)
                                    status_text.text(f"æ­£åœ¨è¯†åˆ«å›¾ç‰‡ {idx+1}/{len(content['images'])}...")
                                    
                                    # ä½¿ç”¨è±†åŒ…APIè¿›è¡Œæ–‡å­—è¯†åˆ«
                                    ocr_text = extract_text_from_image_doubao(img_url, doubao_client, model_id)
                                    
                                    # æ˜¾ç¤ºæ¯å¼ å›¾ç‰‡çš„è¯†åˆ«ç»“æœ
                                    with st.container():
                                        st.markdown(f"#### å›¾ç‰‡ {idx+1} è¯†åˆ«ç»“æœ")
                                        st.text(f"URL: {img_url}")
                                        
                                        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„æ–‡å­—è¯†åˆ«ç»“æœ
                                        is_valid_text = (ocr_text and 
                                                        ocr_text != "æœªè¯†åˆ«åˆ°æ–‡å­—" and 
                                                        not ocr_text.startswith("è±†åŒ…APIè°ƒç”¨é”™è¯¯") and
                                                        not ocr_text.startswith("APIå“åº”ä¸ºç©º") and
                                                        not ocr_text.startswith("è±†åŒ…å®¢æˆ·ç«¯æœªåˆå§‹åŒ–") and
                                                        ocr_text != "æ— æ–‡å­—")
                                        
                                        if is_valid_text:
                                            st.success("âœ… è¯†åˆ«æˆåŠŸ")
                                            st.text_area(f"å›¾ç‰‡{idx+1}æ–‡å­—å†…å®¹", ocr_text, height=150, key=f"ocr_{idx}")
                                            all_ocr_texts.append(f"ã€å›¾ç‰‡{idx+1}ã€‘\n{ocr_text}")
                                        else:
                                            st.warning("âš ï¸ æœªè¯†åˆ«åˆ°æ–‡å­—å†…å®¹")
                                            if ocr_text and (ocr_text.startswith("è±†åŒ…APIè°ƒç”¨é”™è¯¯") or 
                                                           ocr_text.startswith("APIå“åº”ä¸ºç©º") or 
                                                           ocr_text.startswith("è±†åŒ…å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")):
                                                st.error(f"é”™è¯¯ä¿¡æ¯: {ocr_text}")
                                        
                                        st.markdown("---")
                                
                                # æ¸…é™¤è¿›åº¦æ¡
                                progress_bar.empty()
                                status_text.empty()
                                
                                # æ˜¾ç¤ºæ‰€æœ‰OCRç»“æœæ±‡æ€»
                                if all_ocr_texts or content.get('title') or content.get('description'):
                                    st.markdown("### ğŸ“ æ‰€æœ‰æ–‡å­—æ±‡æ€»")
                                    
                                    # æ„å»ºJSONæ ¼å¼çš„æ±‡æ€»å†…å®¹
                                    summary_data = {
                                        "æ ‡é¢˜": content.get('title', ''),
                                        "ç¬”è®°å†…å®¹": content.get('description', ''),
                                        "å›¾ç‰‡æ–‡å­—": {}
                                    }
                                    
                                    # æ·»åŠ å›¾ç‰‡æ–‡å­—è¯†åˆ«ç»“æœ
                                    if all_ocr_texts:
                                        for idx, ocr_text in enumerate(all_ocr_texts):
                                            # æå–å›¾ç‰‡ç¼–å·å’Œæ–‡å­—å†…å®¹
                                            if ocr_text.startswith("ã€å›¾ç‰‡"):
                                                # åˆ†å‰²å‡ºå›¾ç‰‡ç¼–å·å’Œå†…å®¹
                                                lines = ocr_text.split("\n", 1)
                                                img_key = lines[0].replace("ã€", "").replace("ã€‘", "")
                                                img_content = lines[1] if len(lines) > 1 else ""
                                                summary_data["å›¾ç‰‡æ–‡å­—"][img_key] = img_content
                                    
                                    # æ˜¾ç¤ºJSONæ ¼å¼çš„æ±‡æ€»
                                    combined_json = json.dumps(summary_data, ensure_ascii=False, indent=2)
                                    st.code(combined_json, language="json")
                                    
                                    # ä¹Ÿæä¾›æ–‡æœ¬æ ¼å¼çš„é€‰é¡¹
                                    with st.expander("æŸ¥çœ‹æ–‡æœ¬æ ¼å¼æ±‡æ€»"):
                                        text_summary = f"æ ‡é¢˜: {summary_data['æ ‡é¢˜']}\n\n"
                                        text_summary += f"ç¬”è®°å†…å®¹: {summary_data['ç¬”è®°å†…å®¹']}\n\n"
                                        if summary_data['å›¾ç‰‡æ–‡å­—']:
                                            text_summary += "å›¾ç‰‡æ–‡å­—:\n"
                                            for img_key, img_text in summary_data['å›¾ç‰‡æ–‡å­—'].items():
                                                text_summary += f"\nã€{img_key}ã€‘\n{img_text}\n"
                                        st.text_area("æ–‡æœ¬æ ¼å¼æ±‡æ€»", text_summary, height=300)
                                    
                                    combined_text = combined_json
                                    
                                    # å°†OCRç»“æœæ·»åŠ åˆ°å†…å®¹æ•°æ®ä¸­
                                    content['ocr_texts'] = all_ocr_texts
                                    content['combined_ocr_text'] = combined_text
                                    content['summary_data'] = summary_data
                                    
                                    # å¯¼å‡ºåŠŸèƒ½
                                    st.markdown("---")
                                    st.markdown("### ğŸ“¥ å¯¼å‡ºé€‰é¡¹")
                                    
                                    export_col1, export_col2 = st.columns(2)
                                    
                                    with export_col1:
                                        # å¯¼å‡ºæ–‡å­—ä¸ºJSON
                                        if st.button("ğŸ“ å¯¼å‡ºæ±‡æ€»JSON"):
                                            st.download_button(
                                                label="ä¸‹è½½JSONæ±‡æ€»æ–‡ä»¶",
                                                data=combined_text,
                                                file_name=f"xiaohongshu_æ±‡æ€»_{note_id or 'content'}.json",
                                                mime="application/json"
                                            )
                                        
                                        # å¯¼å‡ºæ–‡å­—ä¸ºæ–‡æœ¬æ ¼å¼
                                        if st.button("ğŸ“„ å¯¼å‡ºæ–‡æœ¬æ ¼å¼"):
                                            text_content = f"æ ‡é¢˜: {summary_data['æ ‡é¢˜']}\n\n"
                                            text_content += f"ç¬”è®°å†…å®¹: {summary_data['ç¬”è®°å†…å®¹']}\n\n"
                                            if summary_data['å›¾ç‰‡æ–‡å­—']:
                                                text_content += "å›¾ç‰‡æ–‡å­—:\n"
                                                for img_key, img_text in summary_data['å›¾ç‰‡æ–‡å­—'].items():
                                                    text_content += f"\nã€{img_key}ã€‘\n{img_text}\n"
                                            
                                            st.download_button(
                                                label="ä¸‹è½½æ–‡å­—æ–‡ä»¶",
                                                data=text_content,
                                                file_name=f"xiaohongshu_æ–‡å­—_{note_id or 'content'}.txt",
                                                mime="text/plain"
                                            )
                                    
                                    with export_col2:
                                        # å¯¼å‡ºå®Œæ•´JSON
                                        if st.button("ğŸ“„ å¯¼å‡ºå®Œæ•´æ•°æ®"):
                                            json_str = json.dumps(content, ensure_ascii=False, indent=2)
                                            st.download_button(
                                                label="ä¸‹è½½JSONæ–‡ä»¶",
                                                data=json_str,
                                                file_name=f"xiaohongshu_å®Œæ•´_{note_id or 'content'}.json",
                                                mime="application/json"
                                            )
                                elif not content.get('title') and not content.get('description'):
                                    st.warning("âš ï¸ æ‰€æœ‰å›¾ç‰‡éƒ½æœªèƒ½è¯†åˆ«åˆ°æ–‡å­—å†…å®¹ï¼Œä¸”æœªè·å–åˆ°åŸºæœ¬ä¿¡æ¯")
                            else:
                                st.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡")

# ä½¿ç”¨è¯´æ˜
with st.expander("ğŸ“š ä½¿ç”¨è¯´æ˜"):
    st.markdown("""
    ### å¦‚ä½•ä½¿ç”¨
    1. åœ¨ä¾§è¾¹æ è¾“å…¥æ‚¨çš„è±†åŒ…API Keyå’Œæ¨¡å‹ID
    2. åœ¨å°çº¢ä¹¦ä¸­æ‰¾åˆ°æƒ³è¦æå–æ–‡å­—çš„ç¬”è®°
    3. å¤åˆ¶ç¬”è®°é“¾æ¥ï¼ˆé€šå¸¸ä»¥ https://www.xiaohongshu.com/explore/ å¼€å¤´ï¼‰
    4. å°†é“¾æ¥ç²˜è´´åˆ°ä¸Šæ–¹è¾“å…¥æ¡†ä¸­
    5. ç‚¹å‡»"å¼€å§‹æå–æ–‡å­—"æŒ‰é’®
    6. ç­‰å¾…æ–‡å­—è¯†åˆ«å®Œæˆï¼ŒæŸ¥çœ‹ç»“æœ
    
    ### åŠŸèƒ½ç‰¹è‰² âœ¨
    - **ä¸“ä¸šæ–‡å­—æå–**: ä¸“é—¨ç”¨äºæå–å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹
    - **è±†åŒ…å¤§æ¨¡å‹**: ä½¿ç”¨å…ˆè¿›çš„è§†è§‰å¤§æ¨¡å‹ï¼Œè¯†åˆ«å‡†ç¡®ç‡é«˜
    - **æ‰¹é‡å¤„ç†**: è‡ªåŠ¨å¤„ç†ç¬”è®°ä¸­çš„æ‰€æœ‰å›¾ç‰‡
    - **æ–‡å­—æ±‡æ€»**: å°†æ‰€æœ‰å›¾ç‰‡çš„æ–‡å­—å†…å®¹åˆå¹¶å±•ç¤º
    - **å¤šæ ¼å¼å¯¼å‡º**: æ”¯æŒçº¯æ–‡å­—å’ŒJSONæ ¼å¼å¯¼å‡º
    
    ### æ”¯æŒçš„é“¾æ¥æ ¼å¼
    - https://www.xiaohongshu.com/explore/...
    - https://www.xiaohongshu.com/discovery/item/...
    - å…¶ä»–å°çº¢ä¹¦åˆ†äº«é“¾æ¥
    
    ### æŠ€æœ¯è¯´æ˜
    - ä½¿ç”¨è±†åŒ…è§†è§‰å¤§æ¨¡å‹è¿›è¡Œæ–‡å­—è¯†åˆ«
    - æ”¯æŒä¸­è‹±æ–‡æ··åˆè¯†åˆ«
    - åŸºäºäº‘ç«¯APIï¼Œè¯†åˆ«é€Ÿåº¦å¿«
    - è‡ªåŠ¨å¤„ç†å›¾ç‰‡URLç¼–ç é—®é¢˜
    
    ### æ³¨æ„äº‹é¡¹
    - è¯·éµå®ˆå°çº¢ä¹¦çš„ä½¿ç”¨æ¡æ¬¾
    - ä»…ç”¨äºä¸ªäººå­¦ä¹ å’Œç ”ç©¶ç›®çš„
    - éƒ¨åˆ†å†…å®¹å¯èƒ½å› éšç§è®¾ç½®æ— æ³•æŠ“å–
    - æ–‡å­—è¯†åˆ«å‡†ç¡®æ€§å–å†³äºå›¾ç‰‡è´¨é‡
    - éœ€è¦æœ‰æ•ˆçš„è±†åŒ…API Keyå’Œæ¨¡å‹ID
    """)

# å…è´£å£°æ˜
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666; font-size: 12px;'>
âš ï¸ å…è´£å£°æ˜ï¼šæœ¬å·¥å…·ä»…ä¾›å­¦ä¹ äº¤æµä½¿ç”¨ï¼Œè¯·éµå®ˆç›¸å…³æ³•å¾‹æ³•è§„å’Œå¹³å°è§„åˆ™ã€‚ä½¿ç”¨è€…éœ€è‡ªè¡Œæ‰¿æ‹…ç›¸åº”è´£ä»»ã€‚<br>
ğŸ” æ–‡å­—è¯†åˆ«åŠŸèƒ½åŸºäºè±†åŒ…è§†è§‰å¤§æ¨¡å‹å®ç°ï¼Œè¯†åˆ«ç»“æœä»…ä¾›å‚è€ƒã€‚
</div>
""", unsafe_allow_html=True) 
